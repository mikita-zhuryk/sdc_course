{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SagFccejBwiY"
   },
   "source": [
    "# Домашнее задание 3: Perception\n",
    "\n",
    "В этом задании вам будет необходимо обучить PointNet для задачи фильтрации шума в лидарном облаке.\n",
    "\n",
    "В 3 семинаре мы с вами придумавали руками фичи и пытались обучить на этих данных catboost. Практика показывает, что сетки куда более способные генераторы фичей.\n",
    "\n",
    "[Данные](https://yadi.sk/d/CBoVCVIxJ2q2cw)\n",
    "\n",
    "Задание:\n",
    "\n",
    "1. Необходимо реализовать PointNet, который будет работать на данных со снегом из 3 семинара. PointNet должен работать на окрестностях точек, нет смысла запускать его на всем облаке. PointNet должен включать в себя шаг агрегации по множеству: например с помощью функции максимума, шаг подклеивания агрегированного вектора к исходным точкам и шаг вычисления фичей по отдельным точкам. Вероятно вы захотите повторить эту процедуру несколько раз для улучшения качества. Статья: https://arxiv.org/abs/1612.00593. Вы можете выбрать любой фреймворк для реализации.\n",
    "2. Ваш PointNet должен ограничить сверху размер окрестности. В референсной реализации использовались 64 точки.\n",
    "3. Разбиение на train/test. Для разбиения используйте следующий код.\n",
    "```\n",
    "scene_indices = np.arange(0, 291)\n",
    "np.random.seed(100)\n",
    "np.random.shuffle(scene_indices)\n",
    "train_indices = scene_indices[:260]\n",
    "test_indices = scene_indices[260:]\n",
    "```\n",
    "4. Данные лучше генерировать on-demand, таким образом вам не придется хранить в памяти большие массивы точек. В tensorflow это можно реализовать через tf.data.\n",
    "\n",
    "5. PointNet это функция, которая работает на неупорядоченном множестве точек. В нашем же кейсе мы не хотим предсказать свойство окрестности, мы хотим предсказать свойство точки. Подумайте о том как можно модифицировать архитектуру, чтобы pointnet \"не забывал\" фичи точки, которая нам интересна. (Это поможет улучшить качество)\n",
    "\n",
    "\n",
    "## Формальные требования\n",
    "\n",
    "1. В вашей архитектуре должны быть признаки PointNet: вычисление глобального вектора множества, подклеивание его обратно, вычисление фичей по точкам.\n",
    "\n",
    "2. ROC-AUC на тестовом датасете должен превышать 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yYByeO_Bwiz",
    "outputId": "915841b1-4f84-4043-bfd2-5960bdcff567"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vs2M2yvlCCFc",
    "outputId": "164916e0-ece7-4e68-ed6a-f7250d638939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d4sTCB3WBwi2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import typing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Mh_vJhjBwi4",
    "outputId": "a01d100c-6213-4605-81be-0e70a04f9d48"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# features = pd.read_csv('data/snow_features.csv', index_col=0)\n",
    "features = pd.read_csv('drive/MyDrive/sdc-colab-data/perception/snow_features.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "MZ22onw-Bwi5",
    "outputId": "6a62cb26-eb5a-479b-90bf-2aae4b3ff1bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scene_id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>intensity</th>\n",
       "      <th>ring</th>\n",
       "      <th>label</th>\n",
       "      <th>min_intensity_1.0</th>\n",
       "      <th>max_intensity_1.0</th>\n",
       "      <th>median_intensity_1.0</th>\n",
       "      <th>std_intensity_1.0</th>\n",
       "      <th>min_ring_1.0</th>\n",
       "      <th>max_ring_1.0</th>\n",
       "      <th>median_ring_1.0</th>\n",
       "      <th>std_ring_1.0</th>\n",
       "      <th>r_std_1.0</th>\n",
       "      <th>n_neighbours_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.355618</td>\n",
       "      <td>-4.206962</td>\n",
       "      <td>0.344085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.916535</td>\n",
       "      <td>-1.972164</td>\n",
       "      <td>0.283262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.410451</td>\n",
       "      <td>-2.113039</td>\n",
       "      <td>2.137792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.845870</td>\n",
       "      <td>-1.406652</td>\n",
       "      <td>0.406310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.326218</td>\n",
       "      <td>-0.346060</td>\n",
       "      <td>0.226469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-29.016968</td>\n",
       "      <td>-2.179385</td>\n",
       "      <td>0.945424</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.074985</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.044024</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.730534</td>\n",
       "      <td>16.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.966555</td>\n",
       "      <td>0.192132</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.041912</td>\n",
       "      <td>-0.009894</td>\n",
       "      <td>0.055311</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.730534</td>\n",
       "      <td>16.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.966555</td>\n",
       "      <td>0.189939</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.275961</td>\n",
       "      <td>0.790447</td>\n",
       "      <td>0.086301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.041361</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.290426</td>\n",
       "      <td>1.923754</td>\n",
       "      <td>0.044705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.028832</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scene_id          x         y  ...  std_ring_1.0  r_std_1.0  n_neighbours_1.0\n",
       "0       0.0 -11.355618 -4.206962  ...      0.000000   0.000000               1.0\n",
       "1       0.0  -5.916535 -1.972164  ...      0.000000   0.000000               1.0\n",
       "2       0.0  -7.410451 -2.113039  ...      0.000000   0.000000               1.0\n",
       "3       0.0 -13.845870 -1.406652  ...      0.000000   0.000000               1.0\n",
       "4       0.0  -8.326218 -0.346060  ...      0.000000   0.000000               1.0\n",
       "5       0.0 -29.016968 -2.179385  ...      0.000000   0.000000               1.0\n",
       "6       0.0  -2.074985  0.003017  ...      4.966555   0.192132               3.0\n",
       "7       0.0  -2.041912 -0.009894  ...      4.966555   0.189939               3.0\n",
       "8       0.0  -6.275961  0.790447  ...      3.000000   0.041361               2.0\n",
       "9       0.0  -8.290426  1.923754  ...      2.500000   0.028832               2.0\n",
       "\n",
       "[10 rows x 17 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_gGZLxUiBwi6"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, data_df: pd.DataFrame) -> None:\n",
    "        self.df: pd.DataFrame = data_df.reset_index(drop=True)\n",
    "        self.scene_ids = self.df.scene_id.unique().tolist()\n",
    "        self.n_scenes = len(self.scene_ids)\n",
    "        \n",
    "    def __getitem__(self, scene_idx: int) -> tp.Any:\n",
    "        return self.df[self.df.scene_id == self.scene_ids[scene_idx]]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.n_scenes\n",
    "    \n",
    "    \n",
    "class SceneDataset(Dataset):\n",
    "    def __init__(self, cloud_df: pd.DataFrame) -> None:\n",
    "        self.features = cloud_df.drop(columns=['label', 'scene_id']).to_numpy()\n",
    "        self.tree = KDTree(self.features[:, :3])\n",
    "        self.labels = cloud_df.label.to_numpy()\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> tp.Any:\n",
    "        point = self.features[idx, :3]\n",
    "        neighbor_ids, _ = self.tree.query_radius(point[np.newaxis, ...], r=3,\n",
    "                                                 return_distance=True, sort_results=True)\n",
    "        neighbor_ids = neighbor_ids[0]\n",
    "        n_points = neighbor_ids.size\n",
    "        neighbor_features = self.features[neighbor_ids]\n",
    "        if n_points < 32:\n",
    "            neighbor_features = np.pad(neighbor_features, ((0, 32 - n_points), (0, 0)))\n",
    "        neighbor_features = neighbor_features[:32]\n",
    "        return neighbor_features, self.labels[idx]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NyKAbSRwBwi7"
   },
   "outputs": [],
   "source": [
    "scene_indices = np.arange(0, 291)\n",
    "np.random.seed(100)\n",
    "np.random.shuffle(scene_indices)\n",
    "train_indices = scene_indices[:260]\n",
    "test_indices = scene_indices[260:]\n",
    "\n",
    "train_data = features[features.scene_id.isin(train_indices)]\n",
    "test_data = features[features.scene_id.isin(test_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iyREQlYVBwi8"
   },
   "outputs": [],
   "source": [
    "train_data = PointCloudDataset(train_data)\n",
    "test_data = PointCloudDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Brcy56u-ejYk"
   },
   "outputs": [],
   "source": [
    "class_balance = 1 / train_data.df.groupby('label').count().iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YqLB5VoGBwi8"
   },
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn import metrics as M\n",
    "\n",
    "# Values are (metric, <needs_proba>)\n",
    "NAME_TO_METRIC = {'accuracy': (M.accuracy_score, False),\n",
    "                  'recall': (M.recall_score, False),\n",
    "                  'precision': (M.precision_score, False),\n",
    "                  'f1': (M.f1_score, False),\n",
    "                  'roc_auc': (M.roc_auc_score, True)}\n",
    "\n",
    "\n",
    "class ClassificationMetricLogger:\n",
    "    def __init__(self, n_classes: int, metrics: tp.List[str] = ['precision', 'recall', 'f1'],\n",
    "                 averaging: str = 'weighted') -> None:\n",
    "        self.n_metrics = len(metrics)\n",
    "        self.metrics = metrics\n",
    "        self.train_losses: tp.List[float] = []\n",
    "        self.train_probs: tp.List[float] = []\n",
    "        self.train_preds: tp.List[int] = []\n",
    "        self.train_gt: tp.List[int] = []\n",
    "        self.val_losses: tp.List[float] = []\n",
    "        self.val_probs: tp.List[float] = []\n",
    "        self.val_preds: tp.List[int] = []\n",
    "        self.val_gt: tp.List[int] = []\n",
    "        self._train = True\n",
    "        self.n_classes = n_classes\n",
    "        self.averaging = averaging\n",
    "\n",
    "    def train(self, train: bool = True) -> None:\n",
    "        self._train = train\n",
    "\n",
    "    def eval(self) -> None:\n",
    "        self._train = False\n",
    "        \n",
    "    def __logits_to_probs(self, logits: torch.Tensor) -> tp.List[float]:\n",
    "        return tp.cast(tp.List[float], torch.softmax(logits, dim=1)[:, 1].numpy().astype(float).tolist())\n",
    "\n",
    "    def __logits_to_classes(self, logits: torch.Tensor) -> tp.List[int]:\n",
    "        return tp.cast(tp.List[int], torch.argmax(logits, dim=1).numpy().astype(int).tolist())\n",
    "\n",
    "    def process_predictions(self, preds: torch.Tensor, gt: torch.Tensor, loss: float) -> None:\n",
    "        classes = self.__logits_to_classes(preds)\n",
    "        probs = self.__logits_to_probs(preds)\n",
    "        gt = gt.numpy().tolist()\n",
    "        if self._train:\n",
    "            self.train_losses.append(loss)\n",
    "            self.train_probs.extend(probs)\n",
    "            self.train_preds.extend(classes)\n",
    "            self.train_gt.extend(gt)\n",
    "        else:\n",
    "            self.val_losses.append(loss)\n",
    "            self.val_probs.extend(probs)\n",
    "            self.val_preds.extend(classes)\n",
    "            self.val_gt.extend(gt)\n",
    "\n",
    "    def __metrics(self, train: bool = False) -> tp.Dict[str, float]:\n",
    "        if train:\n",
    "            losses = self.train_losses\n",
    "            probs = self.train_probs\n",
    "            preds = self.train_preds\n",
    "            gt = self.train_gt\n",
    "        else:\n",
    "            losses = self.val_losses\n",
    "            probs = self.val_probs\n",
    "            preds = self.val_preds\n",
    "            gt = self.val_gt\n",
    "\n",
    "        metric_dict = {'mean_loss': float(np.mean(losses))}\n",
    "        for metric in self.metrics:\n",
    "            metric_fn, needs_proba = NAME_TO_METRIC[metric]\n",
    "            if needs_proba:\n",
    "                metric_dict[metric] = float(metric_fn(gt, probs,\n",
    "                                                      labels=np.arange(self.n_classes),\n",
    "                                                      average=self.averaging))\n",
    "            else:\n",
    "                metric_dict[metric] = float(metric_fn(gt, preds,\n",
    "                                                      labels=np.arange(self.n_classes),\n",
    "                                                      average=self.averaging))\n",
    "        return metric_dict\n",
    "\n",
    "    def __describe_split(self, train: bool = True) -> str:\n",
    "        m = self.__metrics(train)\n",
    "        s = ''\n",
    "        for (k, v) in m.items():\n",
    "            s += f'{k}: {v}\\n'\n",
    "        return s\n",
    "\n",
    "    def train_metrics(self) -> tp.Dict[str, float]:\n",
    "        return self.__metrics(train=True)\n",
    "\n",
    "    def val_metrics(self) -> tp.Dict[str, float]:\n",
    "        return self.__metrics(train=False)\n",
    "\n",
    "    def get_summary(self) -> str:\n",
    "        s = 'Train metrics:\\n'\n",
    "        s += self.__describe_split(train=True)\n",
    "        s += 'Val metrics:\\n'\n",
    "        s += self.__describe_split(train=False)\n",
    "        return s\n",
    "\n",
    "    def print_summary(self) -> None:\n",
    "        print(self.get_summary())\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.train_losses = []\n",
    "        self.train_probs = []\n",
    "        self.train_preds = []\n",
    "        self.train_gt = []\n",
    "        self.val_losses = []\n",
    "        self.val_probs = []\n",
    "        self.val_preds = []\n",
    "        self.val_gt = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SsIysm_MBwi-"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PointNetModel(nn.Module):\n",
    "    def __init__(self, in_features: int = 15, n_out_classes: int = 2) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_mlp = nn.Sequential(nn.Linear(in_features, 64),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Linear(64, 64))\n",
    "        clf_mlp_features = 64\n",
    "        get_clf_mlp = lambda: nn.Sequential(nn.Linear(clf_mlp_features + in_features, clf_mlp_features),\n",
    "                                            nn.ReLU())\n",
    "        self.combining_mlps = torch.nn.ModuleList([get_clf_mlp() for _ in range(3)])\n",
    "        self.clf_mlp = nn.Sequential(nn.Linear(clf_mlp_features + in_features, 64),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(64, 16),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(16, n_out_classes))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        inputs = x\n",
    "        global_features = torch.max(self.embedding_mlp(x), dim=1).values.view(batch_size, 1, -1)\n",
    "        for i in range(len(self.combining_mlps)):\n",
    "            x = torch.cat((inputs, torch.tile(global_features, (1, inputs.shape[1], 1))), dim=2)\n",
    "            global_features = torch.max(self.combining_mlps[i](x), dim=1).values.view(batch_size, 1, -1)\n",
    "        prediction = self.clf_mlp(torch.cat((inputs[:, 0], torch.squeeze(global_features, dim=1)), dim=1))\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "udIsOjXhBwi-"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          optimizer: torch.optim.Adam,\n",
    "          train_data: PointCloudDataset,\n",
    "          compute_loss: tp.Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "          metric_logger: ClassificationMetricLogger) -> None:\n",
    "    model.train()\n",
    "    metric_logger.train()\n",
    "    metrics = {}\n",
    "    with tqdm(train_data, desc='Train scenes', position=1) as pbar:\n",
    "        for i, scene_df in enumerate(pbar):\n",
    "            if i == 30:\n",
    "                break  # early stopping because model fits by this time\n",
    "            scene_data = SceneDataset(scene_df)\n",
    "            scene_loader = DataLoader(scene_data, batch_size=512, num_workers=2, shuffle=True)\n",
    "            for (features, gt) in scene_loader:\n",
    "                features = features.float()\n",
    "                gt = gt.long()\n",
    "                optimizer.zero_grad()\n",
    "                pred = model(features.to(DEVICE))\n",
    "                loss = compute_loss(pred, gt.to(DEVICE))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                metric_logger.process_predictions(pred.detach().cpu(), gt, loss.detach().cpu().item())\n",
    "            if i % 15 == 0:\n",
    "                metrics = metric_logger.train_metrics()\n",
    "            pbar.set_postfix(metrics)\n",
    "            \n",
    "            \n",
    "def evaluate(model: torch.nn.Module,\n",
    "             test_data: PointCloudDataset,\n",
    "             compute_loss: tp.Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "             metric_logger: ClassificationMetricLogger) -> None:\n",
    "    model.eval()\n",
    "    metric_logger.eval()\n",
    "    metrics = {}\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_data, desc='Test scenes', position=2) as pbar:\n",
    "            for i, scene_df in enumerate(pbar):\n",
    "                scene_data = SceneDataset(scene_df)\n",
    "                scene_loader = DataLoader(scene_data, batch_size=512, num_workers=2, shuffle=True)\n",
    "                for (features, gt) in scene_loader:\n",
    "                    features = features.float()\n",
    "                    gt = gt.long()\n",
    "                    pred = model(features.to(DEVICE))\n",
    "                    loss = compute_loss(pred, gt.to(DEVICE))\n",
    "                    metric_logger.process_predictions(pred.cpu(), gt, loss.cpu().item())\n",
    "                if i % 5 == 0:\n",
    "                    metrics = metric_logger.val_metrics()\n",
    "            pbar.set_postfix(metrics)\n",
    "                \n",
    "\n",
    "def single_epoch(model: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Adam,\n",
    "                 train_data: PointCloudDataset,\n",
    "                 test_data: PointCloudDataset,\n",
    "                 compute_loss: tp.Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "                 metric_logger: ClassificationMetricLogger) -> None:\n",
    "    train(model, optimizer, train_data, compute_loss, metric_logger)\n",
    "    evaluate(model, test_data, compute_loss, metric_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AEAkoHRLBwjC"
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# DEVICE = 'cpu'\n",
    "N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "d0jt7Y6DBwjD"
   },
   "outputs": [],
   "source": [
    "model = PointNetModel()\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9)\n",
    "metric_logger = ClassificationMetricLogger(n_classes=2,\n",
    "                                           metrics=['precision', 'recall', 'f1', 'roc_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "H9B2PJ7BCW_S"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# cache_path = './'\n",
    "cache_path = 'drive/MyDrive/sdc-colab-data/perception/models/'\n",
    "os.makedirs(cache_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MZSDRjqEBwjE"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "# state_dict = torch.load(os.path.join(cache_path, f'pointnet-{start_epoch - 1}.pth'), map_location='cpu')\n",
    "# model.cpu()\n",
    "# model.load_state_dict(state_dict['model'])\n",
    "# model.to(DEVICE)\n",
    "# optimizer.load_state_dict(state_dict['optimizer'])\n",
    "# scheduler.load_state_dict(state_dict['scheduler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1uFDfG69BwjE"
   },
   "outputs": [],
   "source": [
    "class_weights = torch.from_numpy(class_balance.to_numpy()).float()\n",
    "class_weights /= class_weights.norm()\n",
    "compute_loss = torch.nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gZbeJTJBwjF",
    "outputId": "66f10679-3bcf-40f1-8d78-cef1cfeef205",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train scenes:   0%|          | 0/260 [00:00<?, ?it/s]\u001b[A\n",
      "Train scenes:   0%|          | 0/260 [00:05<?, ?it/s, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   0%|          | 1/260 [00:05<25:13,  5.84s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   0%|          | 1/260 [00:11<25:13,  5.84s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   1%|          | 2/260 [00:11<24:43,  5.75s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   1%|          | 2/260 [00:15<24:43,  5.75s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   1%|          | 3/260 [00:15<22:30,  5.25s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   1%|          | 3/260 [00:21<22:30,  5.25s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   2%|▏         | 4/260 [00:21<23:24,  5.49s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   2%|▏         | 4/260 [00:24<23:24,  5.49s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   2%|▏         | 5/260 [00:24<19:58,  4.70s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   2%|▏         | 5/260 [00:27<19:58,  4.70s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   2%|▏         | 6/260 [00:27<18:25,  4.35s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   2%|▏         | 6/260 [00:31<18:25,  4.35s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   3%|▎         | 7/260 [00:31<17:00,  4.03s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   3%|▎         | 7/260 [00:36<17:00,  4.03s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   3%|▎         | 8/260 [00:36<18:19,  4.36s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   3%|▎         | 8/260 [00:40<18:19,  4.36s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   3%|▎         | 9/260 [00:40<17:40,  4.23s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   3%|▎         | 9/260 [00:44<17:40,  4.23s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   4%|▍         | 10/260 [00:44<17:56,  4.31s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   4%|▍         | 10/260 [00:46<17:56,  4.31s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   4%|▍         | 11/260 [00:46<14:18,  3.45s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   4%|▍         | 11/260 [00:53<14:18,  3.45s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   5%|▍         | 12/260 [00:53<19:38,  4.75s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   5%|▍         | 12/260 [00:59<19:38,  4.75s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   5%|▌         | 13/260 [00:59<19:59,  4.85s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   5%|▌         | 13/260 [01:03<19:59,  4.85s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   5%|▌         | 14/260 [01:03<19:10,  4.68s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   5%|▌         | 14/260 [01:07<19:10,  4.68s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   6%|▌         | 15/260 [01:07<17:54,  4.39s/it, mean_loss=0.201, precision=0.993, recall=0.945, f1=0.968, roc_auc=0.904]\u001b[A\n",
      "Train scenes:   6%|▌         | 15/260 [01:09<17:54,  4.39s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   6%|▌         | 16/260 [01:09<15:52,  3.90s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   6%|▌         | 16/260 [01:13<15:52,  3.90s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   7%|▋         | 17/260 [01:13<15:37,  3.86s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   7%|▋         | 17/260 [01:15<15:37,  3.86s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   7%|▋         | 18/260 [01:15<13:19,  3.30s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   7%|▋         | 18/260 [01:23<13:19,  3.30s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   7%|▋         | 19/260 [01:23<18:45,  4.67s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   7%|▋         | 19/260 [01:28<18:45,  4.67s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   8%|▊         | 20/260 [01:28<18:41,  4.67s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   8%|▊         | 20/260 [01:51<18:41,  4.67s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   8%|▊         | 21/260 [01:51<40:55, 10.27s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   8%|▊         | 21/260 [01:57<40:55, 10.27s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   8%|▊         | 22/260 [01:57<35:54,  9.05s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   8%|▊         | 22/260 [02:01<35:54,  9.05s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   9%|▉         | 23/260 [02:01<29:27,  7.46s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   9%|▉         | 23/260 [02:05<29:27,  7.46s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   9%|▉         | 24/260 [02:05<25:30,  6.49s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:   9%|▉         | 24/260 [02:10<25:30,  6.49s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:  10%|▉         | 25/260 [02:10<23:03,  5.89s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:  10%|▉         | 25/260 [02:15<23:03,  5.89s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:  10%|█         | 26/260 [02:15<22:30,  5.77s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:  10%|█         | 26/260 [02:21<22:30,  5.77s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:  10%|█         | 27/260 [02:21<22:20,  5.75s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:  10%|█         | 27/260 [02:27<22:20,  5.75s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:  11%|█         | 28/260 [02:27<22:38,  5.86s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:  11%|█         | 28/260 [02:30<22:38,  5.86s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:  11%|█         | 29/260 [02:30<19:03,  4.95s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:  11%|█         | 29/260 [02:35<19:03,  4.95s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\u001b[A\n",
      "Train scenes:  12%|█▏        | 30/260 [02:35<19:54,  5.19s/it, mean_loss=0.0478, precision=0.989, recall=0.988, f1=0.988, roc_auc=0.997]\n",
      "\n",
      "\n",
      "Test scenes:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:   3%|▎         | 1/31 [00:04<02:21,  4.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:   6%|▋         | 2/31 [00:10<02:27,  5.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  10%|▉         | 3/31 [00:16<02:31,  5.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  13%|█▎        | 4/31 [00:22<02:27,  5.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  16%|█▌        | 5/31 [00:24<01:54,  4.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  19%|█▉        | 6/31 [00:37<02:57,  7.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  23%|██▎       | 7/31 [00:49<03:26,  8.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  26%|██▌       | 8/31 [01:06<04:10, 10.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  29%|██▉       | 9/31 [01:11<03:22,  9.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  32%|███▏      | 10/31 [01:15<02:39,  7.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  35%|███▌      | 11/31 [01:19<02:12,  6.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  39%|███▊      | 12/31 [01:22<01:44,  5.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  42%|████▏     | 13/31 [01:29<01:46,  5.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  45%|████▌     | 14/31 [01:33<01:33,  5.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  48%|████▊     | 15/31 [01:37<01:17,  4.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  52%|█████▏    | 16/31 [01:44<01:21,  5.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  55%|█████▍    | 17/31 [01:46<01:02,  4.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  58%|█████▊    | 18/31 [01:52<01:03,  4.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  61%|██████▏   | 19/31 [01:59<01:08,  5.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  65%|██████▍   | 20/31 [02:01<00:49,  4.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  68%|██████▊   | 21/31 [02:05<00:42,  4.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  71%|███████   | 22/31 [02:08<00:36,  4.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  74%|███████▍  | 23/31 [02:12<00:31,  3.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  77%|███████▋  | 24/31 [02:15<00:26,  3.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  81%|████████  | 25/31 [02:17<00:18,  3.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  84%|████████▍ | 26/31 [02:22<00:18,  3.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  87%|████████▋ | 27/31 [02:24<00:12,  3.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  90%|█████████ | 28/31 [02:29<00:11,  3.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  94%|█████████▎| 29/31 [02:31<00:06,  3.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes:  97%|█████████▋| 30/31 [02:55<00:09,  9.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "Test scenes: 100%|██████████| 31/31 [03:00<00:00,  5.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [05:41<00:00, 341.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metrics:\n",
      "mean_loss: 0.0445413033889357\n",
      "precision: 0.9898578889618035\n",
      "recall: 0.9890218484494693\n",
      "f1: 0.9892503840959931\n",
      "roc_auc: 0.9972642277622608\n",
      "Val metrics:\n",
      "mean_loss: 0.09404236614698751\n",
      "precision: 0.9867603282300503\n",
      "recall: 0.9863434504878009\n",
      "f1: 0.9864964986867393\n",
      "roc_auc: 0.9962878222091127\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in tqdm(range(start_epoch, N_EPOCHS), desc='Epoch', position=0):\n",
    "    single_epoch(model, optimizer, train_data, test_data, compute_loss, metric_logger)\n",
    "    print(f'Epoch {ep}:')\n",
    "    metric_logger.print_summary()\n",
    "    scheduler.step()\n",
    "    state_dict = {'model': model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict(),\n",
    "                  'scheduler': scheduler.state_dict(),\n",
    "                  'epoch': ep}\n",
    "    torch.save(state_dict, os.path.join(cache_path, f'pointnet-{ep}.pth'))\n",
    "    metric_logger.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-jZcnk5qBwjF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sdc-perception.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
